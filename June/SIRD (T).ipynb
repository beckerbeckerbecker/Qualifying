{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This code sets up and trains a deep learning model to learn the parameters of a SIRD (Susceptible, Infected, Recovered, Deceased) model for the spread of a disease, like COVID-19, in a population.\n\nThe SIRD model is a compartmental model that represents the number of people in a population that are susceptible (S), infected (I), recovered (R), and deceased (D). It includes parameters for the rates at which individuals move from one compartment to another: \n- β is the transmission rate \n- γ is the recovery rate \n- δ is the mortality rate\n\nThe code does the following:\n\n1. It imports necessary libraries and packages, including TensorFlow, NumPy, Pandas, Scikit-learn, Scipy, Matplotlib, and others.\n2. It sets the directory paths and creates a new directory for storing figures, if it doesn't already exist.\n3. It sets the seed for the random number generators in NumPy and TensorFlow to ensure the reproducibility of the results.\n4. It defines a class named `SIRD` that represents the SIRD model. \n\n   - The `SIRD` class has methods for initializing the model (creating the neural network), defining the model's dynamics in `net_sird`, defining parameters in `net_param`, calculating residuals in `net_residual`, training the model, and making predictions.\n\n5. It reads the dataset named 'tndata.csv' and preprocesses the data into the form that can be input into the SIRD model.\n\n6. It creates an instance of the `SIRD` class and trains it.\n\n7. After the model has been trained, it makes predictions for the parameters and compartments.\n\n8. It calculates error metrics like RMSE (Root Mean Squared Error), MAPE (Mean Absolute Percentage Error), and EV (Explained Variance) to evaluate the performance of the model.\n\nThe SIRD model is implemented as a Physics-informed Neural Network (PINN), where the physical laws governing the disease spread (the SIRD equations) are used as a part of the loss function to guide the training of the model. This approach allows the model to learn not only from the data but also from the underlying physical laws, improving the generalization capability and interpretability of the model.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import tensorflow.compat.v1 as tf  #Torku\ntf.disable_v2_behavior()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom scipy import optimize\nimport matplotlib.dates as dates\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import explained_variance_score\n# from ipywidgets import interact, widgets\nfrom scipy.integrate import solve_ivp\nplt.style.use('seaborn-poster')\nmatplotlib.rcParams['figure.figsize'] = (10., 6.)\nimport copy\n# import sympy\n# %matplotlib inline\nimport scipy as sp\nfrom scipy.integrate import odeint\nimport datetime as dt\nimport timeit\nimport time\nimport os\nimport sys\nimport time\nsys.path.insert(0, '../../Utilities/')\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom scipy.interpolate import CubicSpline\nnp.set_printoptions(threshold=np.inf)\nfrom prettytable import PrettyTable  \nimport matplotlib as mpl\nimport matplotlib.dates as mdates\n\nc_dir =os.getcwd()\npath = '/Figures/'\nout = c_dir +path\nif not os.path.exists(out):\n    os.makedirs(out)\n\nnp.random.seed(12345)\ntf.set_random_seed(12345)\n#tf.compat.v1.disable_eager_execution()\nclass SIRD:\n    def __init__(self, I, R, S,D, T,N0, layers):\n        self.t = T\n        self.I = I\n        self.R = R\n        self.S = S\n        self.D = D\n        self.layers = layers\n        self.N = N0\n        self.lb = T.min()\n        self.ub = T.max()\n        # self.gamma = tf.Variable([1], constraint=lambda x: tf.abs(x),dtype=tf.float32)\n        # Initialize NN\n        self.weights1, self.biases1 = self.initialize_NN(self.layers)\n        self.weights2, self.biases2 = self.initialize_NN(self.layers)\n        self.weights3, self.biases3 = self.initialize_NN(self.layers)\n        self.weights4, self.biases4 = self.initialize_NN(self.layers)\n        self.weights5, self.biases5 = self.initialize_NN(self.layers)\n        self.weights6, self.biases6 = self.initialize_NN(self.layers)\n        self.weights7, self.biases7 = self.initialize_NN(self.layers)\n        self.weights8, self.biases8 = self.initialize_NN(self.layers)\n        # tf placeholders and graph\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True))\n        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n        self.I_tf = tf.placeholder(tf.float32, shape=[None, self.I.shape[1]])\n        self.R_tf = tf.placeholder(tf.float32, shape=[None, self.R.shape[1]])\n        self.S_tf = tf.placeholder(tf.float32, shape=[None, self.S.shape[1]])\n        self.D_tf = tf.placeholder(tf.float32, shape=[None, self.D.shape[1]])\n        \n        #lossess\n        self.total_loss =[]\n        self.loss_data =[]\n        self.loss_phys =[]\n        self.total_time=[]\n        \n        self.S_pred, self.I_pred, self.R_pred,self.D_pred =self.net_sird(self.t_tf)\n        \n        self.beta_pred,self.gamma_pred, self.delta_pred, self.compliance_pred =self.net_param(self.t_tf)\n        \n        self.e1, self.e2, self.e3, self.e4, self.e5 = self.net_residual(self.t_tf)\n        \n        self.lossPhy = tf.reduce_mean(tf.abs(self.e1)) + tf.reduce_mean(tf.abs(self.e2)) +\\\n                        tf.reduce_mean(tf.abs(self.e3)) + tf.reduce_mean(tf.abs(self.e4)) +\\\n                        tf.reduce_mean(tf.abs(self.e5))\n        self.S_loss = 0\n        self.I_loss = 0\n        self.R_loss = 0\n        self.D_loss = 0\n        iter = 0\n        for i in range(len(T)):\n            if T[i]%1 == 0:\n                self.S_loss += tf.abs(self.S_tf[iter] - self.S_pred[i])\n                self.I_loss += tf.abs(self.I_tf[iter] - self.I_pred[i])\n                self.R_loss += tf.abs(self.R_tf[iter] - self.R_pred[i])\n                self.D_loss += tf.abs(self.D_tf[iter] - self.D_pred[i])\n                iter += 1\n        self.lossData = (self.I_loss + self.R_loss + self.S_loss+self.D_loss)/iter\n        self.loss = self.lossData + self.lossPhy\n        '''\n        self.loss = tf.reduce_mean(tf.abs(self.I_tf - self.I_pred)) + tf.reduce_mean(tf.abs(self.R_pred - self.R_tf)) +\\\n                    tf.reduce_mean(tf.abs(self.S_tf - self.S_pred)) +\\\n                    tf.reduce_mean(tf.abs(self.E1_pred))+ tf.reduce_mean(tf.abs(self.E2_pred)) +\\\n                    tf.reduce_mean(tf.abs(self.E3_pred)) + tf.reduce_mean(tf.abs(self.E4_pred)) \n        '''\n        \n        self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n        # Optimizers\n        '''\n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n                                                                method = 'L-BFGS-B', \n                                                                options = {'maxiter': 50000,\n                                                                           'maxfun': 50000,\n                                                                           'maxcor': 50,\n                                                                           'maxls': 50,\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})    \n        '''                                                                                          \n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n         \n        \n    def initialize_NN(self, layers):        \n        weights = []\n        biases = []\n        num_layers = len(layers)\n        \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases\n    \n    def xavier_init(self, size):\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n    \n    def neural_net(self, X, weights, biases):\n        num_layers = len(weights) + 1\n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.nn.softplus(tf.add(tf.matmul(H, W), b))\n        return Y\n\n    def neural_net1(self, X, weights, biases):\n        num_layers = len(weights) + 1\n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.nn.sigmoid(tf.add(tf.matmul(H, W), b))\n        return Y\n    \n    def net_sird(self, t):\n        NN=self.N\n        S = self.neural_net1(t, self.weights1, self.biases1)\n        I = self.neural_net1(t, self.weights2, self.biases2)\n        R = self.neural_net1(t, self.weights3, self.biases3)\n        D = self.neural_net1(t, self.weights4, self.biases4)\n        return S, I, R, D\n    \n    def net_param(self, t):\n        NN=self.N\n        beta= self.neural_net1(t, self.weights5, self.biases5)\n        gamma = self.neural_net1(t,self.weights6,self.biases6)\n        delta = self.neural_net1(t,self.weights7,self.biases7)\n        compliance = self.neural_net1(t,self.weights8,self.biases8)\n        return beta, gamma, delta, compliance\n        \n\n    def net_residual(self, t):\n        NN=self.N  \n        S, I, R, D =self.net_sird(t)\n        # print(S.shape)\n        beta, gamma, delta, compliance =self.net_param(t) \n        St = tf.gradients(S, t)[0]\n        It = tf.gradients(I, t)[0]  \n        Rt = tf.gradients(R, t)[0]\n        Dt = tf.gradients(D, t)[0]\n        \n        # print(St.shape)\n        \n        e1 = St + beta*(1.0-compliance)*S*I\n        e2 = It - beta*(1.0-compliance)*S*I + gamma*I +delta*I\n        e3 = Rt - gamma*I\n        e4 = Dt - delta*I\n        e5 = 1.0 - (S+I+R+D)\n        return e1, e2, e3, e4, e5\n\n    def callback(self, loss):\n        print('Loss: %.3e' % (loss))\n    def train(self, nIter):\n        tf_dict = {self.t_tf: self.t, self.I_tf: self.I,self.R_tf: self.R,self.S_tf: self.S, self.D_tf: self.D}\n                   \n        start_time = time.time()\n        for it in range(nIter):\n            self.sess.run(self.train_op_Adam, tf_dict)\n            loss_t = self.sess.run(self.loss, tf_dict)\n            loss_d = self.sess.run(self.lossData, tf_dict)\n            loss_p = self.sess.run(self.lossPhy, tf_dict)\n            self.total_loss.append(loss_t)\n            self.loss_data.append(loss_d)\n            self.loss_phys.append(loss_p)\n            elapsed = time.time() - start_time\n            self.total_time.append(elapsed)\n            # Print\n            if it % 500 == 0:\n                elapsed = time.time() - start_time\n                loss_t = self.sess.run(self.loss, tf_dict)\n                # loss_d = self.sess.run(self.lossData, tf_dict)\n                # loss_p = self.sess.run(self.lossPhy, tf_dict)\n                print('It: %d, Loss: %.3e, Time: %.2f' % \n                      (it, loss_t,elapsed))\n                start_time = time.time()\n        '''\n        self.optimizer.minimize(self.sess,\n                                feed_dict = tf_dict,\n                                fetches = [self.loss],\n                                loss_callback = self.callback)\n        '''              \n    def predict(self, t_star):\n        tf_dict = {self.t_tf: t_star}\n        S = self.sess.run(self.S_pred, tf_dict)\n        I = self.sess.run(self.I_pred, tf_dict)\n        R = self.sess.run(self.R_pred, tf_dict)\n        D = self.sess.run(self.D_pred, tf_dict)\n        beta = self.sess.run(self.beta_pred,tf_dict)\n        gamma = self.sess.run(self.gamma_pred,tf_dict)\n        delta = self.sess.run(self.delta_pred,tf_dict)\n        compliance = self.sess.run(self.compliance_pred,tf_dict)\n        return S,I,R,D, beta, gamma, delta, compliance\n    \n##prepare data\n\ndata =pd.read_csv(\"tndata.csv\")\ndef data_preprocess(data, lb, ub, N0):\n    tdat=data.reindex(index=data.index[::-1])\n    ic = tdat[\"TOTAL_CASES\"]\n    dc = tdat[\"TOTAL_DEATHS\"]\n    re = tdat[\"TOTAL_INACTIVE_RECOVERED\"]\n    y1, y2 =np.array(ic.values).reshape((-1,1)), np.array(dc.values).reshape((-1,1))\n    y3   =np.array(re.values).reshape((-1,1))\n    y4 =y1-y3-y2 #infected cases\n    I, R, D =y1[lb:ub,:],y3[lb:ub,:], y2[lb:ub,:]\n    S =N0-I-R-D\n    length =int(ub-lb)\n    T = np.arange(0,length).reshape(length,1)\n    return S, I, R, D, T\n\nN0 = 6.82*1.e6\nlb =19 #change this\nub =280\nlength=int(ub-lb)\n# eps =[1e-1, 1e-2, 1e-3, 1e-5, 1e-7, 1e-9]\nSS, II, RR, DD, T=data_preprocess(data, lb, ub, N0)\nS_original=SS/N0\nI_original=II/N0\nR_original=RR/N0\nD_original=DD/N0\n\nl=4\nneuron =[32,64]\nT = np.arange(0,length - 0.05,0.05)#here I used Nt with 0.1 stepsize\nT = T.reshape(len(T),1)\nfor i in range(2):\n    layers =[1] +l*[neuron[i]] +[1]\n    model = SIRD(II/N0,RR/N0,SS/N0,DD/N0, T,1,layers)\n    model.train(40000)\n    #predict output\n    TT = np.arange(0,length).reshape(length,1)\n    s_p,i_p,r_p,d_p,beta_p, gamma_p, delta_p, compliance_p= model.predict(TT)\n    total_time =sum(model.total_time)\n    ##Get errors\n    I_or =I_original.reshape(-1,)\n    I_p  =i_p.reshape(-1,)\n    test_actual=I_or*N0\n    test_pred =I_p*N0\n    rmse =np.sqrt(mean_squared_error(test_actual, test_pred))\n    mape =np.linalg.norm((test_pred-test_actual),2)/np.linalg.norm(test_actual, 2)\n    ev =1- (np.var(test_pred-test_actual)/np.var(test_actual))\n    rel =np.sum((test_actual-test_pred)**2/(test_actual**2))\n    #Error Metrics\n    with open(out+'cpupinn_{}_{}.txt'.format(l,neuron[i]), 'w') as f:\n        print(\"Error metrics for  layers ={} with fixed neuron ={}\".format(l,neuron[i]), file=f)\n        print(\"=======================================================\\n\", file=f)\n        print('Total CPU Time',total_time , file=f)\n        print('RMSE',rmse, file=f)\n        print('MAPE',mape, file=f)\n        print('EV', ev, file=f)\n        print('REL', rel, file=f)\n        print(\"=======================================================\\n\", file=f)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This code sets up a physics-informed neural network (PINN) to estimate parameters in an SIR model, which is a mathematical model to understand the dynamics of infectious diseases. \n\nThe SIR model is an acronym for 'Susceptible', 'Infected', and 'Recovered', which are the three compartments the model considers. It relies on a set of differential equations that relate these compartments to each other, and uses two key parameters - beta (the infection rate) and gamma (the recovery rate). \n\nThe neural network structure is defined in the class `sir_param`. You have a number of attributes for the class such as the initial conditions of the disease (`s0`, `i0`, `r0`), the size of the population (`N`), the infection and recovery rates (`beta` and `gamma`), the learning rate (`v`), and the data split ratio (`split`). \n\nThen, it defines various methods, such as `xzavier` for Xavier initialization of the weights, `w_b` to initialize weights and biases, `network` and `sir_network` to define the neural network, and `residual_sir` to calculate the residuals of the SIR model. \n\nThe key to the PINN approach here is in the `residual_sir` method, where the network predictions are plugged into the SIR equations. The residuals are then minimized during the training process to ensure that the network solutions are consistent with the underlying SIR model.\n\nFinally, it defines a `train` method for training the model and a `predict` method to get the predictions from the model.\n\nHowever, the code seems to use tensorflow version 1 (`tensorflow.compat.v1`) where you have disabled version 2 behaviour at the beginning (`tff.disable_v2_behavior()`). Starting from TensorFlow 2.0, eager execution is enabled by default, which allows a more interactive frontend to TensorFlow. You might want to refactor your code to use the newer and more streamlined APIs in TensorFlow 2.x, which are generally easier to work with. \n\nFurthermore, for better readability and organization, consider separating the network creation, loss function definition, and training steps into different methods or even different classes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#EINN_model.py\n\n### Using PINN for parameter Estimates\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow.compat.v1 as tff\ntff.disable_v2_behavior()\nimport time\nstart_time = time.time()\ntff.set_random_seed(1234)\nclass sir_param:\n    def __init__(self, n_layers, i,t, lb, ub,bi,gi, v, eta, tf, U0, N, split):\n        ##Initialize \n        self.i, self.t, self.v, self.eta =i, t,v, eta\n        self.lb, self.ub, self.N =lb, ub, N\n        self.n_layers =n_layers\n        self.tf =tf\n        self.split =split\n        indx =int(self.split*len(i))\n        ##Training \n        self.s0, self.i0, self.r0 =U0[0], U0[1], U0[2]\n        self.i_train =self.i[:indx]\n        self.t_train =self.t[:indx]\n        ##Testing\n        self.i_test =self.i[indx:]\n        self.t_test =self.t[indx:]\n        self.i0t =self.i_test[0:1,:]\n        self.r0t =np.array([[0.0]])\n        self.s0t =self.N-self.i0t- self.r0t\n        self.s0t =self.s0t.reshape((-1,1))\n        self.weights, self.biases =self.w_b(self.n_layers)\n        self.beta =tff.Variable([bi], dtype=tff.float32)\n        self.gamma =tff.Variable([gi], dtype=tff.float32)\n        ##Get placeholders as containers for input and output\n        self.sess =tff.Session(config=tff.ConfigProto(allow_soft_placement=True,\n                                                     log_device_placement=True))\n        self.t_t =tff.placeholder(tff.float32, [None, 1])\n        self.i_t =tff.placeholder(tff.float32, [None, 1])\n        self.s0_t =tff.placeholder(tff.float32, [None, 1])\n        self.i0_t =tff.placeholder(tff.float32, [None, 1])\n        self.r0_t =tff.placeholder(tff.float32, [None, 1])\n        ##Placeholders and data containers\n        self.tf_t =tff.placeholder(tff.float32, [None, 1])\n        self.s_pred, self.i_pred, self.r_pred=self.sir_network(self.t_t)\n#         self.beta_pred = self.nn_beta(self.t_t)\n        self.s0_pred =self.s_pred[0]\n        self.i0_pred =self.i_pred[0]\n        self.r0_pred =self.r_pred[0]\n        self.e1,self.e2, self.e3=self.residual_sir(self.tf_t)\n         # Loss: Initial Data\n        self.lossUU0 = tff.reduce_mean(tff.square(self.s0_t - self.s0_pred)) + \\\n            tff.reduce_mean(tff.square(self.i0_t - self.i0_pred)) + \\\n            tff.reduce_mean(tff.square(self.r0_t - self.r0_pred)) \n        ##Data\n        self.lossD = tff.reduce_mean(tff.square(self.i_t-self.i_pred))                   \n        ##Residual\n        self.lossR= tff.reduce_mean(tff.square(self.e1-0.0))+\\\n                    tff.reduce_mean(tff.square(self.e2-0.0))+\\\n                    tff.reduce_mean(tff.square(self.e3-0.0))\n                    \n               \n        \n        self.loss =self.lossUU0+self.lossD+self.lossR\n        self.opt =tff.train.AdamOptimizer().minimize(self.loss)\n    \n        init =tff.global_variables_initializer()\n        self.sess.run(init)                  \n        ##Initialize weights and biases\n    def xzavier(self,dim):\n        d1,d2 =dim[0], dim[1]\n        std =np.sqrt(2.0/(d1+d2))\n        return tff.Variable(tff.truncated_normal([d1,d2], stddev=std), dtype=tff.float32)\n    ##Apply to all wights\n    def w_b(self, n_layers):\n        l=n_layers\n        weights =[self.xzavier([l[j], l[j+1]]) for j in range(0, len(l)-1)]\n        biases =[tff.Variable(tff.zeros([1, l[j+1]], dtype =tff.float32),dtype=tff.float32) for j in range(0,len(l)-1)]\n        return weights,biases\n   \n    #Define the neural network\n    def network(self,t, weights, biases):\n        M=len(weights)+1\n        z=2.0*(t-self.lb)/(self.ub-self.lb)-1.0\n        for i in range(0, M-2):\n             z =tff.nn.tanh(tff.matmul(z, weights[i])+biases[i])\n        y_pred =tff.nn.softplus(tff.matmul(z, weights[-1])+biases[-1])\n        return y_pred\n    \n    def sir_network(self,t):\n        out =self.network(t, self.weights, self.biases)\n        s, i, r =out[:,0:1], out[:,1:2], out[:,2:3]\n        return s, i, r\n\n    \n    def residual_sir(self,t):\n        beta, gamma, v, eta =self.beta, self.gamma, self.v, self.eta\n        s, i,r =self.sir_network(t)\n        s_t =tff.gradients(s, t, unconnected_gradients='zero')[0]\n        i_t =tff.gradients(i, t, unconnected_gradients='zero')[0]\n        r_t =tff.gradients(r, t, unconnected_gradients='zero')[0]\n        N=self.N\n        e1 =s_t +(beta*s*i)/N +v*eta*s\n        e2 =i_t -(beta*s*i)/N +gamma*i\n        e3 =r_t -gamma*i-v*eta*s\n        return e1, e2, e3\n    def callbacks(self, loss, beta, gamma):\n        print('Loss: {}, beta: {}, gamma: {}'.format(loss,beta,gamma))   \n    def train(self, epochs):\n        train_dic ={self.t_t: self.t_train, self.i_t:self.i_train,  self.tf_t:self.tf,\n                   self.s0_t:self.s0, self.i0_t:self.i0, self.r0_t:self.r0}\n        test_dic ={self.t_t: self.t_test, self.i_t:self.i_test,  self.tf_t:self.tf,\n                   self.s0_t:self.s0t, self.i0_t:self.i0t, self.r0_t:self.r0t}\n        start_time = time.time()\n        for i in range(epochs+1):\n            self.sess.run(self.opt, train_dic)\n            self.sess.run(self.opt, test_dic)\n            if i%100==0:\n                elapsed = time.time() - start_time\n                loss_v= self.sess.run(self.loss, train_dic)\n                loss_v1= self.sess.run(self.loss, test_dic)\n                beta_v=self.sess.run(self.beta)\n                gamma_v=self.sess.run(self.gamma)\n                print('Epoch: %d, Train Loss:%.3e, Test Loss:%.3e, beta: %.2f, gamma: %.2f, Time: %.2f'%(i, loss_v, loss_v1, beta_v, gamma_v,elapsed))\n                start_time = time.time()\n    def predict(self,t_hat):\n        tr_dic={self.t_t: t_hat}\n        s_hat =self.sess.run(self.s_pred, tr_dic)\n        i_hat =self.sess.run(self.i_pred, tr_dic)\n        r_hat =self.sess.run(self.r_pred, tr_dic)\n        return s_hat, i_hat, r_hat ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtff\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tff\u001b[38;5;241m.\u001b[39mdisable_v2_behavior()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ],
          "output_type": "error"
        }
      ]
    }
  ]
}